{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /Users/darka/Desktop/tln_repo/tln-1920/part3/exercise2/src/exercise2.py\n",
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    It reads che definition's CSV\n",
    "\n",
    "    :return: four list containing the read definitions.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as content:\n",
    "        cnt = csv.reader(content, delimiter=';')\n",
    "\n",
    "        dictionary = {}\n",
    "        i = 0\n",
    "        for line in cnt:\n",
    "            dictionary[i] = line\n",
    "            i += 1\n",
    "\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(definition):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes stopwords, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing stopwords\n",
    "    definition = definition.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_synset(synset):\n",
    "    \"\"\"\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "    pre_synset = synset.split(\".\")\n",
    "    clean_synset = pre_synset[0]\n",
    "    return clean_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genus_noun(concept_definitions_dict):\n",
    "   # Loading the content-to-form.csv file   \n",
    "    for index in concept_definitions_dict:\n",
    "        hyponyms_list = []\n",
    "\n",
    "        for definition in content[index]:\n",
    "            local_genus = {}\n",
    "            hyponyms = []\n",
    "\n",
    "            def_tokens = word_tokenize(definition)\n",
    "            results = nltk.pos_tag(def_tokens)\n",
    "\n",
    "            possibles_genus = list(filter(lambda x: x[1] == \"NN\", results))\n",
    "            # Eg.: [('abstract', 'NN'), ('concept', 'NN'), ('idea', 'NN'), ('fairness', 'NN'),\n",
    "            # ('front', 'NN'), ('code', 'NN'), ('community', 'NN')]\n",
    "\n",
    "            for g in possibles_genus:\n",
    "                if not g[0] in local_genus:\n",
    "                    local_genus[g[0]] = 1\n",
    "                else:\n",
    "                    local_genus[g[0]] += 1\n",
    "\n",
    "            if len(local_genus) > 0:\n",
    "                genus = max(local_genus, key=local_genus.get)\n",
    "                syns = wn.synsets(genus)\n",
    "\n",
    "                # We take every hyponyms for the Genus of the definition\n",
    "                for i, s in enumerate(syns, start=0):\n",
    "                    hypon = lambda s: s.hyponyms()\n",
    "                    all_hypon = list(s.closure(hypon, depth=1)) \n",
    "                    hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "\n",
    "            hyponyms_list.append(' '.join(hyponyms))\n",
    "\n",
    "        # CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "        # - k is the number of sentences,\n",
    "        # - n is the number of unique words in all sentences combined.\n",
    "        # If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "        vectorizer = CountVectorizer()\n",
    "        matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "\n",
    "        feature_list = vectorizer.get_feature_names()\n",
    "        vectors = matrix.toarray()\n",
    "\n",
    "        # Index of the element with maximum frequency across all the definition of a concept\n",
    "        m = vectors.sum(axis=0).argmax()\n",
    "\n",
    "        print(\"\\t{} - {} - {}\".format(index + 1, feature_list[m], m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genus_hyper(definitions_dict):\n",
    "    content = definitions_dict  # Loading the content-to-form.csv file\n",
    "\n",
    "    for index in content:\n",
    "        genus_dict = {}\n",
    "        hyponyms_list = []\n",
    "\n",
    "        for definition in content[index]:\n",
    "            hypernyms = []\n",
    "            hyponyms = []\n",
    "            clean_tokens = preprocess(definition)\n",
    "\n",
    "            for word in clean_tokens:\n",
    "                syn = [lesk(definition, word)]\n",
    "                if len(syn) > 0:  # needed because for some words lesks returns an empty list\n",
    "                    for s in syn:\n",
    "                        if s:\n",
    "                            hyper = lambda s: s.hypernyms()\n",
    "                            all_hyper = list(s.closure(hyper, depth=1)) \n",
    "                            hypernyms.extend([x.name().split(\".\")[0] for x in all_hyper])\n",
    "\n",
    "                    for g in hypernyms:\n",
    "                        if g not in genus_dict:\n",
    "                            genus_dict[g] = 1\n",
    "                        else:\n",
    "                            genus_dict[g] += 1\n",
    "\n",
    "            if len(genus_dict) > 0:\n",
    "                genus = max(genus_dict, key=genus_dict.get)\n",
    "                syns = wn.synsets(genus)\n",
    "\n",
    "                # We take every hyponyms for the Genus of the definition\n",
    "                for i, s in enumerate(syns, start=0):\n",
    "                    hypon = lambda s: s.hyponyms()\n",
    "                    all_hypon = list(s.closure(hypon, depth=1))\n",
    "                    hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "\n",
    "            hyponyms_list.append(' '.join(hyponyms))\n",
    "\n",
    "        # CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "        # - k is the number of sentences,\n",
    "        # - n is the number of unique words in all sentences combined.\n",
    "        # If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "        vectorizer = CountVectorizer()\n",
    "        matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "\n",
    "        feature_list = vectorizer.get_feature_names()\n",
    "        vectors = matrix.toarray()\n",
    "\n",
    "        # Index of the element with maximum frequency across all the definition of a concept\n",
    "        m = vectors.sum(axis=0).argmax()\n",
    "\n",
    "        print(\"\\t{} - {} - {}\".format(index + 1, feature_list[m], m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genus Noun:\n",
      "\t1 - thing - 388\n",
      "\t2 - skill - 159\n",
      "\t3 - wish - 256\n",
      "\t4 - operation - 216\n",
      "\t5 - subject - 309\n",
      "\t6 - land - 141\n",
      "\t7 - land - 126\n",
      "\t8 - base_alloy - 35\n",
      "\n",
      "Genus Hyper:\n",
      "\t1 - darkness - 0\n",
      "\t2 - focus - 20\n",
      "\t3 - cash - 15\n",
      "\t4 - s_law - 279\n",
      "\t5 - hard_time - 30\n",
      "\t6 - artificial_intelligence - 0\n",
      "\t7 - object - 77\n",
      "\t8 - artificial_intelligence - 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    " \n",
    "    concept_definitions_dict= load_data(\"/Users/darka/Desktop/tln_repo/tln-1920/part3/exercise2/input/content-to-form.csv\")\n",
    "    \n",
    "    print(\"Genus Noun:\")\n",
    "    genus_noun(concept_definitions_dict)\n",
    "    print(\"\\nGenus Hyper:\")\n",
    "    genus_hyper(concept_definitions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
