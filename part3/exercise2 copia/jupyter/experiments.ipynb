{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# IPERONIMI\n",
    "syns = wn.synsets(\"equality.n.02\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hypernyms()  # SOPRA-NOME, categoria superiore della parola\n",
    "    temp = list(s.closure(hyper, depth=3))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('status.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"equality.n.02\").hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balance', 'equatability', 'equivalence', 'evenness', 'isometry', 'egality', 'tie']\n"
     ]
    }
   ],
   "source": [
    "# IPONIMI\n",
    "syns = wn.synsets(\"equality\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "    temp = list(s.closure(hyper, depth=1))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quality of being just or fair'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"justice.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edible_fruit.n.01\n",
      "pome.n.01\n"
     ]
    }
   ],
   "source": [
    "h = wn.synset(\"apple.n.01\").hypernyms()\n",
    "for v in h:\n",
    "    print(v.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a fleshy fruit (apple or pear or related fruits) having seed chambers and an outer fleshy part'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"pome.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    It reads che definition's CSV\n",
    "    :return: four list containing the read definitions.\n",
    "    \"\"\"\n",
    "    with open(options[\"output\"] + 'content-to-form.csv', \"r\", encoding=\"utf-8\") as content:\n",
    "        cnt = csv.reader(content, delimiter=';')\n",
    "\n",
    "        dictionary = {}\n",
    "        i = 0\n",
    "        for line in cnt:\n",
    "            dictionary[i] = line\n",
    "            i += 1\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "def preprocess(definition):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes stopwords, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing stopwords\n",
    "    definition = definition.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "def preprocess_synset(synset):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes the stopword, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "    pre_synset = synset.split(\".\")\n",
    "    clean_synset = pre_synset[0]\n",
    "    return clean_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/content-to-form.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ac0faa93681f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     }\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loading the content-to-form.csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n",
      "\u001b[0;32m<ipython-input-9-c152ba969384>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfour\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mread\u001b[0m \u001b[0mdefinitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'content-to-form.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/content-to-form.csv'"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "        \"output\": \"/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/\",\n",
    "    }\n",
    "\n",
    "content = load_data()  # Loading the content-to-form.csv file\n",
    "\n",
    "'''\n",
    "1. prendo definzione, disambiguo con pos-tagging. il primo nome è il genus\n",
    "2. come approccio personalizzato, teveno un dizionario di genus (dopo aver esplorato tutte le definizioni) e espandevo solo il genus più frequente\n",
    "riducendo la ricerca\n",
    "3. prendo da wordnet i synsets di quel sostantivo, e per ognuno di essi parto in basso con gli iponimi\n",
    "4. calcolo l'iponimo dell'iponimo dell'iponimo..., per non sclerare utilizza la closure (chiusura trasitiva). Calcolo gli iponimi fino a un certo \n",
    "livello\n",
    "5. calcola iponimo con più overlapping, stili classifica\n",
    "'''\n",
    "\n",
    "for index in content:\n",
    "#for index in range(1):\n",
    "    \n",
    "    hyponyms_list = []\n",
    "    \n",
    "    for definition in content[index]:\n",
    "    #for definition in content[0]:\n",
    "        genus_dict = {}\n",
    "        hyponyms = []\n",
    "            \n",
    "        def_tokens = word_tokenize(definition)\n",
    "        results = nltk.pos_tag(def_tokens)\n",
    "        \n",
    "        possibles_genus = list(filter(lambda x: x[1] == \"NN\", results))\n",
    "        # Es.: [('abstract', 'NN'), ('concept', 'NN'), ('idea', 'NN'), ('fairness', 'NN'), ('front', 'NN'), ('code', 'NN'), ('community', 'NN')]\n",
    "\n",
    "        for g in possibles_genus:\n",
    "            if not g[0] in genus_dict:\n",
    "                genus_dict[g[0]] = 1\n",
    "            else:\n",
    "                genus_dict[g[0]] += 1\n",
    "    \n",
    "#         print(index, genus)\n",
    "#         print(\"{} - {}\\n\".format(index, genus_dict))\n",
    "        \n",
    "        if len(genus_dict) > 0:\n",
    "            genus = max(genus_dict, key=genus_dict.get)\n",
    "#             print(\"GENUS: \" + genus)\n",
    "        \n",
    "            syns = wn.synsets(genus)\n",
    "        \n",
    "            # Prendiamo tutti gli iponimi per il genus della singola definizione\n",
    "            for i, s in enumerate(syns, start=0):\n",
    "                hypon = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "                all_hypon = list(s.closure(hypon, depth=1))  # TODO: aumentare a 2,3\n",
    "                hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "#                 print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "#             print(index, hyponyms, \"\\n\")\n",
    "#         else:\n",
    "#             print(\"NADA\")\n",
    "            \n",
    "        \n",
    "        hyponyms_list.append(' '.join(hyponyms))\n",
    "    \n",
    "#     print(hyponyms_list)\n",
    "\n",
    "        \n",
    "    '''\n",
    "    CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "    - k is the number of sentences,\n",
    "    - n is the number of unique words in all sentences combined.\n",
    "    If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "    \n",
    "    feature_list = vectorizer.get_feature_names()\n",
    "    vectors = matrix.toarray()\n",
    "    \n",
    "    m = vectors.sum(axis=0).argmax()\n",
    "    \n",
    "    print(m)\n",
    "    print(feature_list[m] + '\\n')\n",
    "#     print(feature_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/content-to-form.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5839b895a992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loading the content-to-form.csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m '''\n",
      "\u001b[0;32m<ipython-input-9-c152ba969384>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfour\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mread\u001b[0m \u001b[0mdefinitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'content-to-form.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/content-to-form.csv'"
     ]
    }
   ],
   "source": [
    "# Da NOMI a IPERONIMI\n",
    "\n",
    "options = {\n",
    "        \"output\": \"/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/\",\n",
    "    }\n",
    "\n",
    "content = load_data()  # Loading the content-to-form.csv file\n",
    "\n",
    "'''\n",
    "1. prendo definzione, disambiguo con pos-tagging. il primo nome è il genus\n",
    "2. come approccio personalizzato, teveno un dizionario di genus (dopo aver esplorato tutte le definizioni) e espandevo solo il genus più frequente\n",
    "riducendo la ricerca\n",
    "3. prendo da wordnet i synsets di quel sostantivo, e per ognuno di essi parto in basso con gli iponimi\n",
    "4. calcolo l'iponimo dell'iponimo dell'iponimo..., per non sclerare utilizza la closure (chiusura trasitiva). Calcolo gli iponimi fino a un certo \n",
    "livello\n",
    "5. calcola iponimo con più overlapping, stili classifica\n",
    "'''\n",
    "\n",
    "for index in content:\n",
    "#for index in range(1):\n",
    "    \n",
    "    genus_dict = {}\n",
    "    \n",
    "    for definition in content[index]:\n",
    "    #for definition in content[0]:\n",
    "\n",
    "        hypernyms = []\n",
    "        clean_tokens = preprocess(definition)\n",
    "        \n",
    "        all_synsets = []\n",
    "        for word in clean_tokens:\n",
    "            syn = [lesk(definition, word)] # TODO: disambiguare le parole della definizione con lesk e usare i loro synsets per trovare gli iperonimi!\n",
    "            if len(syn) > 0:\n",
    "                for s in syn:\n",
    "                    if s:\n",
    "                        hyper = lambda s: s.hypernyms()\n",
    "                        all_hyper = list(s.closure(hyper, depth=2))  # TODO: aumentare a 2,3\n",
    "                        hypernyms.extend([x.name().split(\".\")[0] for x in all_hyper])\n",
    "\n",
    "                for g in hypernyms:\n",
    "                    if not g in genus_dict:\n",
    "                        genus_dict[g] = 1\n",
    "                    else:\n",
    "                        genus_dict[g] += 1\n",
    "                \n",
    "        # ------------------------------\n",
    "        \n",
    "#         print(genus_dict)\n",
    "    \n",
    "        if len(genus_dict) > 0:\n",
    "            genus = max(genus_dict, key=genus_dict.get)\n",
    "#             print(\"\\n{}\\n\".format(genus))\n",
    "\n",
    "            syns = wn.synsets(genus)\n",
    "\n",
    "            # Prendiamo tutti gli iponimi per il genus della singola definizione\n",
    "            for i, s in enumerate(syns, start=0):\n",
    "                hypon = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "                all_hypon = list(s.closure(hypon, depth=1))  # TODO: aumentare a 2,3\n",
    "                hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "#             print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "#             print(index, hyponyms, \"\\n\")\n",
    "#         else:\n",
    "#             print(\"NADA\")\n",
    "\n",
    "        hyponyms_list.append(' '.join(hyponyms))\n",
    "\n",
    "#         print(hyponyms_list)\n",
    "\n",
    "\n",
    "    '''\n",
    "    CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "    - k is the number of sentences,\n",
    "    - n is the number of unique words in all sentences combined.\n",
    "    If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "    '''\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "\n",
    "    feature_list = vectorizer.get_feature_names()\n",
    "    vectors = matrix.toarray()\n",
    "\n",
    "    m = vectors.sum(axis=0).argmax()\n",
    "\n",
    "    print(m)\n",
    "    print(feature_list[m] + '\\n')\n",
    "#     print(feature_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\nn_webserver\")\n",
    "\n",
    "from employee import motivation_to_work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
